{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 4, 5, 5)\n",
      "(8, 4, 3, 3)\n",
      "(1, 8, 5, 5)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "b = 1\n",
    "c = 4\n",
    "w = 5\n",
    "h = 5\n",
    "o = 8\n",
    "s = 3\n",
    "\n",
    "img = np.random.rand(b, c, w, h)\n",
    "filter = np.random.rand(o, c, s, s)\n",
    "output = np.zeros((b, o, w, h))\n",
    "\n",
    "def conv_img(img, filter):\n",
    "    c, w, h = img.shape\n",
    "    o, c, s, _ = filter.shape\n",
    "    shift = s // 2\n",
    "    output = np.zeros((o, w, h))\n",
    "    padded_img = np.pad(img, [(0, 0), (shift, shift), (shift, shift)], 'constant')\n",
    "    \n",
    "    for i in range(shift, w + shift):\n",
    "        for j in range(shift, h + shift):\n",
    "            for k in range(c):\n",
    "                for l in range(s):\n",
    "                    for m in range(s):\n",
    "                        for n in range(o):\n",
    "                            output[n, i - shift, j - shift] += padded_img[k, i + l - shift, j + m - shift] * filter[n, k, l, m]\n",
    "    return output\n",
    "\n",
    "for i in range(b):\n",
    "    output[i, :, :, :] = conv_img(img[i], filter)\n",
    "\n",
    "b = 1\n",
    "c = 4\n",
    "w = 5\n",
    "h = 5\n",
    "o = 8\n",
    "s = 3\n",
    "\n",
    "img = np.random.rand(b, c, w, h)\n",
    "filter = np.random.rand(o, c, s, s)\n",
    "output = np.zeros((b, o, w, h))\n",
    "print(img.shape)\n",
    "print(filter.shape)\n",
    "\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_size(input_tensor, output_c, kernel_size, stride, padding):\n",
    "    b, c, w, h = input_tensor.shape\n",
    "    new_w  = (w - kernel_size + 2 * padding) // stride + 1\n",
    "    new_h  = (h - kernel_size + 2 * padding) // stride + 1\n",
    "    \n",
    "    return (b, output_c, new_w, new_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfold(img, kernel_size, stride, padding):\n",
    "    img = np.pad(img, [(0, 0), (0, 0), (padding, 1), (padding, padding)], 'constant')\n",
    "    b, c, h, w = img.shape\n",
    "    output_w  = (w - kernel_size) // stride + 1\n",
    "    output_h  = (h - kernel_size) // stride + 1\n",
    "    shift = kernel_size // 2\n",
    "    patches = output_w * output_h\n",
    "    values_per_patch = kernel_size * kernel_size * c\n",
    "    output = np.zeros((b, values_per_patch, patches))\n",
    "\n",
    "    for batch in range(b):\n",
    "        for i in range(shift, w - shift, stride):\n",
    "            for j in range(shift, h - shift, stride):\n",
    "                for k in range(kernel_size):\n",
    "                    for l in range(kernel_size):\n",
    "                        for m in range(c):\n",
    "                            id0 = batch\n",
    "                            id1 = kernel_size * kernel_size * m + l * kernel_size + k\n",
    "                            id2 = (i - shift) * output_w + (j - shift)\n",
    "                            output[id0, id1, id2] = img[batch, m, i + k - shift, j + l - shift]\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 27, 36])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Conv2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):\n",
    "        super(Conv2D, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "        self.out_channels = out_channels\n",
    "        w_init = torch.randn(in_channels * kernel_size * kernel_size, out_channels)\n",
    "        # w_init = torch.randn(out_channels, in_channels * kernel_size * kernel_size)\n",
    "        self.W = nn.Parameter(w_init)\n",
    "        self.W_r = self.W.transpose(0, 1).reshape(out_channels, in_channels, kernel_size, kernel_size)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        x = F.unfold(input_batch, kernel_size=self.kernel_size, stride=self.stride, padding=self.padding)\n",
    "        x = x.transpose(1, 2).reshape(-1, self.W.shape[0])\n",
    "        x = torch.matmul(x, self.W)\n",
    "        b, _, h, w = self.get_expected_shape(input_batch)\n",
    "        x = x.reshape(b, h, w, self.out_channels).transpose(2, 3).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    def get_expected_shape(self, input_tensor):\n",
    "        b, _, w, h = input_tensor.shape\n",
    "        new_h  = (h - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        new_w  = (w - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
    "        \n",
    "        return (b, self.out_channels, new_w, new_h)\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 5\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = kernel_size // 2\n",
    "\n",
    "conv = Conv2D(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "input_batch = torch.randn(1, 3, 16, 16)\n",
    "output_batch = conv(input_batch)\n",
    "out_c = torch.nn.functional.conv2d(input_batch, conv.W_r, stride=stride, padding=padding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "[1,  2000] loss: 2.382\n",
      "[1,  4000] loss: 2.233\n",
      "[1,  6000] loss: 2.198\n",
      "[1,  8000] loss: 2.169\n",
      "[1, 10000] loss: 2.118\n",
      "[1, 12000] loss: 2.101\n",
      "EPOCH 1\n",
      "Accuracy of the network on the 10000 test images: 21.88%\n",
      "Loss of the network on the 10000 test images: 1038.670\n",
      "[2,  2000] loss: 2.074\n",
      "[2,  4000] loss: 2.068\n",
      "[2,  6000] loss: 2.045\n",
      "[2,  8000] loss: 2.042\n",
      "[2, 10000] loss: 2.028\n",
      "[2, 12000] loss: 2.016\n",
      "EPOCH 2\n",
      "Accuracy of the network on the 10000 test images: 25.75%\n",
      "Loss of the network on the 10000 test images: 990.091\n",
      "[3,  2000] loss: 2.004\n",
      "[3,  4000] loss: 1.995\n",
      "[3,  6000] loss: 1.985\n",
      "[3,  8000] loss: 1.971\n",
      "[3, 10000] loss: 1.979\n",
      "[3, 12000] loss: 1.973\n",
      "EPOCH 3\n",
      "Accuracy of the network on the 10000 test images: 28.03%\n",
      "Loss of the network on the 10000 test images: 983.774\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 78\u001b[0m\n\u001b[1;32m     76\u001b[0m outputs \u001b[38;5;241m=\u001b[39m net(inputs)\n\u001b[1;32m     77\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 78\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     79\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# print statistics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DARL2/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/DARL2/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 4\n",
    "\n",
    "\n",
    "LR = 0.0001\n",
    "EPOCH = 10\n",
    "MOMENTUM = 0.9\n",
    "BATCH_SIZE = 4\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2D(3, 6, 5)\n",
    "        self.conv2 = Conv2D(6, 16, 5)\n",
    "        # self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        # self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "        self.pool_res = nn.MaxPool2d(5, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except batch\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "net = Net()\n",
    "\n",
    "\n",
    "results = {\n",
    "    'Epoch': [],\n",
    "    'Loss': [],\n",
    "    'Accuracy': [],\n",
    "    'Method': [],\n",
    "    'Run': []\n",
    "}\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE,\n",
    "                                        shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                    download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=BATCH_SIZE,\n",
    "                                        shuffle=False, num_workers=0)\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "optimizer = optim.SGD(net.parameters(), lr=LR, momentum=MOMENTUM)\n",
    "\n",
    "for i in range(10):\n",
    "    for layer in net.children():\n",
    "        if hasattr(layer, 'reset_parameters'):\n",
    "            layer.reset_parameters()\n",
    "    for epoch in range(EPOCH):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(trainloader, 0): # get the inputs; data is a list of [inputs, labels] \n",
    "            inputs, labels = data # zero the parameter gradients\n",
    "            optimizer.zero_grad() # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "                print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "                running_loss = 0.0\n",
    "        correct = 0.0\n",
    "        total = 0.0\n",
    "        # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "        with torch.no_grad():\n",
    "            for data in testloader:\n",
    "                images, labels = data\n",
    "                # calculate outputs by running images through the network\n",
    "                outputs = net(images)\n",
    "                # the class with the highest energy is what we choose as prediction\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'EPOCH {epoch + 1}')\n",
    "        print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')\n",
    "        print(f'Loss of the network on the 10000 test images: {running_loss:.3f}')\n",
    "\n",
    "        # results['Loss'].append(running_loss)\n",
    "        # results['Epoch'].append(epoch)\n",
    "        # results['Accuracy'].append(correct / total)\n",
    "        # results['Method'].append('Residual')\n",
    "        # results['Run'].append(i)\n",
    "\n",
    "# df= pd.DataFrame.from_dict(results)\n",
    "# df.to_csv('data_12_residual.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 3, 3])\n",
      "tensor(0.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class Conv2DFunc(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input_batch, kernel, stride=1, padding=1):\n",
    "        o, _, k, _ = kernel.shape\n",
    "        b, _, new_h, new_w = Conv2DFunc.get_expected_shape(input_batch, k, padding, stride)\n",
    "        w = kernel.reshape(o, -1).transpose(0, 1)\n",
    "        ctx.input_shape = input_batch.shape\n",
    "        ctx.kernel_shape = kernel.shape\n",
    "        ctx.conv_params = (stride, padding)\n",
    "        ctx.w = w\n",
    "\n",
    "        x = F.unfold(input_batch, kernel_size=k, stride=stride, padding=padding)\n",
    "        x = x.transpose(1, 2)\n",
    "        ctx.folded_shape = x.shape\n",
    "        x = x.reshape(-1, w.shape[0])\n",
    "        ctx.u = x\n",
    "        x = torch.matmul(x, w)\n",
    "        x = x.reshape(b, new_h, new_w, o)\n",
    "        x = x.transpose(2, 3).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        grad = grad_output.transpose(1, 2).transpose(2, 3)\n",
    "        grad = grad.reshape(-1, ctx.kernel_shape[0])\n",
    "        w_grad = torch.matmul(ctx.u.transpose(0, 1), grad)\n",
    "        w_grad = w_grad.transpose(0, 1)\n",
    "        w_grad = w_grad.reshape(ctx.kernel_shape)\n",
    "\n",
    "        grad = torch.matmul(grad, ctx.w.transpose(0, 1)) # U\n",
    "        grad = grad.reshape(ctx.folded_shape) # U'\n",
    "        grad = grad.transpose(1, 2) # U''\n",
    "\n",
    "        out_size = (ctx.input_shape[2], ctx.input_shape[3])\n",
    "        kernel_size = ctx.kernel_shape[2]\n",
    "        stride = ctx.conv_params[0]\n",
    "        padding = ctx.conv_params[1]\n",
    "        input_grad = F.fold(grad, output_size=out_size, kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "\n",
    "        return input_grad, w_grad, None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def get_expected_shape(input_tensor, kernel_size, padding, stride):\n",
    "        b, _, w, h = input_tensor.shape\n",
    "        new_h  = (h - kernel_size + 2 * padding) // stride + 1\n",
    "        new_w  = (w - kernel_size + 2 * padding) // stride + 1\n",
    "        \n",
    "        return (b, out_channels, new_w, new_h)\n",
    "\n",
    "torch.manual_seed(2137)\n",
    "\n",
    "in_channels = 3\n",
    "out_channels = 5\n",
    "kernel_size = 3\n",
    "stride = 1\n",
    "padding = kernel_size // 2\n",
    "\n",
    "kernel = torch.rand(out_channels, in_channels, kernel_size, kernel_size, requires_grad=True)\n",
    "conv = Conv2DFunc\n",
    "input_batch = torch.randn(1, 3, 16, 16, requires_grad=True)\n",
    "\n",
    "output_batch = torch.nn.functional.conv2d(input_batch, kernel, stride=stride, padding=padding)\n",
    "loss = torch.sum(output_batch)\n",
    "loss.backward()\n",
    "grad1 = kernel.grad\n",
    "# print(grad1.shape)\n",
    "# print(kernel.grad)\n",
    "\n",
    "output_batch = conv.apply(input_batch, kernel, stride, padding)\n",
    "loss = torch.sum(output_batch)\n",
    "loss.backward()\n",
    "grad2 = kernel.grad\n",
    "# print(grad2)\n",
    "print(kernel.grad.shape)\n",
    "print(torch.sum(grad1 - grad2))\n",
    "\n",
    "# output_grad = torch.ones(output_batch.shape)\n",
    "# conv.backward(output_grad)\n",
    "# torch.autograd.gradcheck(torch.nn.functional.conv2d, {\"input\": input_batch, \"weight\": kernel, \"stride\": stride, \"padding\": padding})\n",
    "# print(torch.max(torch.abs(out_c - output_batch)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DARL2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
